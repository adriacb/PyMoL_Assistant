{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(os.path.join(os.getcwd(), os.pardir, os.pardir))\n",
    "# from src.logger import Logger\n",
    "# from src.models import QuestionModel\n",
    "from src.prompts import *\n",
    "from src.db.vector_store import load_config, load_vector_store\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_qdrant import Qdrant\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.schema import (\n",
    "    SystemMessage,    # Generic system message\n",
    "    HumanMessage,     # HumanMessage is a message from the human\n",
    "    AIMessage,        # AIMessage is a message from the AI\n",
    ")\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import MessagesState, END, START, StateGraph\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "from typing import Annotated, Literal, TypedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qdrant': {'collection_name': 'pymol-assistant', 'url': 'https://f01bac7d-144a-44f8-b643-17d9c6db29f0.us-east4-0.gcp.cloud.qdrant.io:6333', 'vector_config': {'size': 768, 'distance': 'cosine'}, 'vector_name': 'content', 'content_payload_key': 'content'}, 'embeddings': {'model': 'text-embedding-3-small'}, 'data': {'to_process': '../../prep/data/pymol-docs.csv', 'parquet_file': '../../prep/data/pymol-docs.parquet'}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "config = load_config()\n",
    "print(config)\n",
    "embeddings = OpenAIEmbeddings(model=config.get('embeddings').get('model'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDRANT_URL:  https://f01bac7d-144a-44f8-b643-17d9c6db29f0.us-east4-0.gcp.cloud.qdrant.io:6333\n",
      "QDRANT_COLLECTION_NAME:  pymol-assistant\n"
     ]
    }
   ],
   "source": [
    "qdrant = load_vector_store(config=config, embeddings=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG toolkit\n",
    "@tool\n",
    "def query_pymol_docs(query: str, qdrant: Qdrant, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Query the PyMOL documentation using Qdrant\n",
    "\n",
    "    Args:\n",
    "        query (str): The query to search for\n",
    "        qdrant (Qdrant): The Qdrant instance\n",
    "        top_k (int, optional): The number of results to return. Defaults to 5.\n",
    "    \n",
    "    Returns:\n",
    "        List[Dict]: The top k results for the query with the highest cosine similarity score\n",
    "    \"\"\"\n",
    "    # Get the top k results for the query\n",
    "    results = qdrant.similarity_search_with_score(query=query, k=top_k)\n",
    "\n",
    "    context: str = \"\"\n",
    "\n",
    "    for doc, score in results:\n",
    "        d = f\"* [SIM={score:3f}] {doc.page_content} [{doc.metadata}]\"\n",
    "        context += d + \"\\n\"\n",
    "    \n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = query_pymol_docs(\"how to load a pdb file\", qdrant=qdrant, top_k=4)\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [query_pymol_docs]\n",
    "tool_node = ToolNode(tools=tools, name='tools')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", END]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    # If the LLM makes a tool call, then we route to the \"tools\" node\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    # Otherwise, we stop (reply to the user)\n",
    "    return END\n",
    "\n",
    "# Define the function that calls the model\n",
    "def call_model(state: MessagesState):\n",
    "    messages = state['messages']\n",
    "    response = model.invoke(messages)\n",
    "    # We return a list, because this will get added to the existing list\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the Nodes and the Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "\n",
    "# Define the two nodes we will cycle between\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", tool_node)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "# This means that this node is the first one called\n",
    "workflow.add_edge(START, \"agent\")\n",
    "\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "# This means that after `tools` is called, `agent` node is called next.\n",
    "workflow.add_edge(\"tools\", 'agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize memory to persist state between graph runs\n",
    "checkpointer = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we're (optionally) passing the memory when compiling the graph\n",
    "app = workflow.compile(checkpointer=checkpointer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rdk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
